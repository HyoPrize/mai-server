{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFPsZzzoD1Fv",
        "outputId": "9c7e6d95-d3e8-45a5-d4b8-2c92ba051ba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (4.9.2)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[K     |████████████████████████████████| 465 kB 79.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from JPype1>=0.7.0->konlpy) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->JPype1>=0.7.0->konlpy) (3.0.9)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from konlpy.tag import Okt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import load_model\n",
        "import json"
      ],
      "metadata": {
        "id": "bO_HWlUCEi_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = load_model('/content/universe/hyol_model3.h5')"
      ],
      "metadata": {
        "id": "vEo_gVs_EmKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()\n",
        "stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다','로','것','고','원']\n",
        "max_len = 100"
      ],
      "metadata": {
        "id": "bR3pcoQVEzZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/universe/tokenizer.pickle','rb') as handle:\n",
        "  tokenizer = pickle.load(handle)"
      ],
      "metadata": {
        "id": "vMZ2_LS1E35s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_and_token(sentense):\n",
        "  new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', sentense)\n",
        "  word_array = okt.morphs(new_sentence, stem=True) \n",
        "  word_array = [word for word in word_array if not word in stopwords] \n",
        "  token_array = tokenizer.texts_to_sequences([word_array])\n",
        "  return word_array, token_array[0]"
      ],
      "metadata": {
        "id": "J1KSukZcE9kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_new_json(input_json_path, output_json_path):\n",
        "  with open(input_json_path, 'r') as input_json_file:\n",
        "    place_data = json.load(input_json_file)\n",
        "\n",
        "    # delete non-review place\n",
        "    place_data = list(filter(lambda place: len(place[\"placeReviews\"]) > 0, place_data))\n",
        "\n",
        "    # update review to [review, token]\n",
        "    for place in place_data:\n",
        "      place[\"placeWordCount\"] = {}\n",
        "      temp_reviews = place[\"placeReviews\"].copy()\n",
        "      temp_review_starts = place[\"placeReviewStar\"].copy()\n",
        "      for idx, review in enumerate(place[\"placeReviews\"]):\n",
        "        if len(review) > 0:\n",
        "          word_array, token_array = get_word_and_token(review)\n",
        "          temp_reviews[idx] = {\n",
        "              \"review\": review,\n",
        "              \"token\": token_array\n",
        "          }\n",
        "          for word in word_array:\n",
        "            try:\n",
        "              place[\"placeWordCount\"][word] += 1\n",
        "            except:\n",
        "              place[\"placeWordCount\"][word] = 1\n",
        "\n",
        "      place[\"placeWordCount\"] = dict(sorted(place[\"placeWordCount\"].items(), key=lambda x:x[1], reverse = True))\n",
        "\n",
        "      # delete empty review\n",
        "      place[\"placeReviews\"] = list(filter(lambda review: len(review) > 0, temp_reviews))\n",
        "\n",
        "      del(place[\"placeTotalReview\"])\n",
        "      del(place[\"placeReviewStar\"])\n",
        "\n",
        "    with open(output_json_path, 'w', encoding=\"UTF-8\") as output_json_file:\n",
        "      json.dump(place_data, output_json_file, indent=4, ensure_ascii=False)\n",
        "      \n",
        "      "
      ],
      "metadata": {
        "id": "gI_iYUnmLG5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_word_and_token(\"더럽고 최악. 불편함. 불편함 그리고 직원들 싸가지없음\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mN8G5E4sKm3f",
        "outputId": "fed48154-b014-421c-e9d1-8f8d39448db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['더럽다', '최악', '불편하다', '불편하다', '그리고', '직원', '싸가지', '없다']\n",
            "[[417, 120, 199, 199, 107, 12, 337, 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  print(\"맛집 start\")\n",
        "  write_new_json(\"/content/universe/output_맛집.json\", \"/content/universe/output_맛집_new.json\")\n",
        "  print(\"맛집 end\")\n",
        "  print(\"치킨 start\")\n",
        "  write_new_json(\"/content/universe/output_치킨.json\", \"/content/universe/output_치킨_new.json\")\n",
        "  print(\"치킨 end\")\n",
        "  print(\"카페 start\")\n",
        "  write_new_json(\"/content/universe/output_카페.json\", \"/content/universe/output_카페_new.json\")\n",
        "  print(\"카페 end\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3IgpsqQM0xb",
        "outputId": "c21631d4-a830-47e6-9265-b29e2f3a3af5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "맛집 start\n",
            "맛집 end\n",
            "치킨 start\n",
            "치킨 end\n",
            "카페 start\n",
            "카페 end\n"
          ]
        }
      ]
    }
  ]
}